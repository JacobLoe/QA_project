{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code to download SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "#each paragraph is a different topic indicated by the title\n",
    "# each topic contains a number of different short passages related to the topic, the context\n",
    "# for each topic there are ~10 question/answer pairs, and an indicator if a question is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['version', 'data'])\n"
     ]
    }
   ],
   "source": [
    "print(train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'paragraphs'])\n",
      "(442,)\n"
     ]
    }
   ],
   "source": [
    "print(train['data'][0].keys())\n",
    "print(np.shape(train['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train['data'][0]['paragraphs'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,question in enumerate(train['data'][0]['paragraphs'][1]['qas']):\n",
    "#     print(i,question)\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['qas', 'context'])\n",
      "(66,)\n"
     ]
    }
   ],
   "source": [
    "# b\n",
    "print(train['data'][0]['paragraphs'][0].keys())\n",
    "print(np.shape(train['data'][0]['paragraphs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['question', 'id', 'answers', 'is_impossible'])\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(train['data'][0]['paragraphs'][0]['qas'][0].keys())\n",
    "print(np.shape(train['data'][0]['paragraphs'][0]['qas'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(train['data'][0]['paragraphs'][0]['context'])\n",
    "print(np.shape(train['data'][0]['paragraphs'][0]['context']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When did Beyonce start becoming popular?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['data'][0]['paragraphs'][0]['qas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_paragraph=train['data'][0]['paragraphs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_paragraph.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        for qas in paragraph['qas']:#train_test_paragraph['qas']:\n",
    "            if (qas['is_impossible']==False):\n",
    "                train_new['context'].append(context)\n",
    "                train_new['question'].append(tokenize(qas['question']))\n",
    "                train_new['answer'].append(tokenize(qas['answers'][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['data'][0]['paragraphs'][0]['qas'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['data'][4]['paragraphs'][0]['qas'][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for context,question,answer in train_new['context'] + train_new['question'] + train_new['answer']:\n",
    "#     print(context)\n",
    "#     print(question)\n",
    "#     print(answer)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [a[0]]+[b[0]]+[c[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n",
      "20\n",
      "{\"['y', 'q']\", 'c', 'y', \"['x', 'w']\", \"['v', 'r']\", 'b', 'v', 's', 'd', \"['b', 't']\", 'f', 't', 'x', 'a', 'r', 'w', 'q', 'e', 'g', \"['c', 'e']\"}\n"
     ]
    }
   ],
   "source": [
    "# test =set()\n",
    "a=['q','w','e','r','t']#,'z','u','i','o','p','ü']\n",
    "b=['a','s','d','f','g']#,'h','j','k','l','ö','ä']\n",
    "d=['y','x','c','v','b']#,'n','m','1','2','3','4']\n",
    "e=['5','6','7','8','9']\n",
    "c = [[q,w] for q,w in zip(d,a)]\n",
    "# print(c)\n",
    "\n",
    "for q,w,e in zip(a,b,c):\n",
    "#     print(test)\n",
    "    test |= set([q]+[w]+[str(e)])\n",
    "print(type(test))\n",
    "print(len(test))\n",
    "print(test)\n",
    "# # print(sorted(test))\n",
    "# # for e in c:\n",
    "# #     test|=set(e)\n",
    "# # print(type(test))\n",
    "# # print(len(test))\n",
    "# # print(sorted(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106825,)\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for context,question,answer in zip(train_new['context'],train_new['question'],train_new['answer']):\n",
    "#     print(type(context))\n",
    "#     print(type(question))\n",
    "#     print(type(answer))\n",
    "    vocab |= set(context+question+[str(answer)])\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1\n",
    "print(np.shape(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "story_maxlen = max(map(len, (x for x, _, _ in zip(train_new['context'],train_new['question'],train_new['answer']))))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in zip(train_new['context'],train_new['question'],train_new['answer']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_idx={}\n",
    "# for i,c in enumerate(vocab):\n",
    "#     word_idx[c]=i+1\n",
    "# #     break\n",
    "# print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "152\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(word_idx))\n",
    "print(story_maxlen)\n",
    "print(query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86821,)"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_new['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106825\n"
     ]
    }
   ],
   "source": [
    "print(len(word_idx))\n",
    "# print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def asd(data, word_idx, story_maxlen, query_maxlen):\n",
    "#     xs = []\n",
    "#     xqs = []\n",
    "#     ys = []\n",
    "#     for story, query, answer in data:\n",
    "#         x = [word_idx[w] for w in story]\n",
    "#         xq = [word_idx[w] for w in query]\n",
    "#         # let's not forget that index 0 is reserved\n",
    "#         y = np.zeros(len(word_idx) + 1)\n",
    "#     print(np.shape(x))\n",
    "#     print(x)\n",
    "#     print(np.shape(xq))\n",
    "# #     print(xq)\n",
    "# #     print(y)\n",
    "# asd(zip(train_new['context'],train_new['question'],train_new['answer']),\n",
    "#                                 word_idx,\n",
    "#                                 story_maxlen,\n",
    "#                                 query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for story, query, answer in data:\n",
    "        x = [word_idx[w] for w in story]\n",
    "        xq = [word_idx[w] for w in query]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = np.zeros(len(word_idx) + 1)\n",
    "        try:\n",
    "            p=word_idx[str(answer)]\n",
    "        except:\n",
    "            print(answer)\n",
    "        y[word_idx[str(answer)]] = 1\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return (pad_sequences(xs, maxlen=story_maxlen),\n",
    "            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded_train=vectorize_stories(zip(train_new['context'],train_new['question'],train_new['answer']),\n",
    "#                                 word_idx,\n",
    "#                                 story_maxlen,\n",
    "#                                 query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
