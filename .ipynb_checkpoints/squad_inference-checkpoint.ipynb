{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import json\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import process_data as pd\n",
    "#https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 100)    3329500     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    1282800     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 100), (None, 80400       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 100), (None, 80400       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 100)    853900      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 200)          0           lstm_1[0][1]                     \n",
      "                                                                 lstm_2[0][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200)          0           lstm_1[0][2]                     \n",
      "                                                                 lstm_2[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, None, 200)    240800      embedding_3[0][0]                \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8539)   1716339     lstm_3[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,584,139\n",
      "Trainable params: 2,117,939\n",
      "Non-trainable params: 5,466,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "json_file=open('models/baseline/baseline_model.json','r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.summary()\n",
    "# load weights into new model\n",
    "loaded_model.load_weights('models/baseline/baseline_model.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86821\n",
      "86821\n",
      "86821\n",
      "length of vocab:  33295 12828 8539\n",
      "max length of data: 499 30 43\n",
      "length of data:  10000 10000 10000\n",
      "shape of the input for the model:  (10000, 499) (10000, 30) (10000, 43) (10000, 43, 8539)\n",
      "dict_keys(['len_vocab', 'token_to_int', 'decoder_input', 'encoder_input'])\n"
     ]
    }
   ],
   "source": [
    "#open SQuAD-dataset and extract the relevant data from the json-file\n",
    "#to a easier readable/accessible dictionary\n",
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        context=paragraph['context']\n",
    "        for qas in paragraph['qas']:\n",
    "            #create a dataset with only the answerable questions\n",
    "            #add a bos and eos token to the target\n",
    "            if (qas['is_impossible']==False):\n",
    "                a=context.lower()\n",
    "                b=qas['question'].lower()\n",
    "                c=qas['answers'][0]['text'].lower()\n",
    "                \n",
    "                train_new['context'].append('\\t'+a+'\\n')\n",
    "                train_new['question'].append('\\t'+b+'\\n')\n",
    "                train_new['answer'].append('\\t'+c+'\\n')\n",
    "print(len(train_new['context']))\n",
    "print(len(train_new['question']))\n",
    "print(len(train_new['answer']))\n",
    "size=10000\n",
    "context=train_new['context'][:size]\n",
    "question=train_new['question'][:size]\n",
    "answer=train_new['answer'][:size]\n",
    "data=[context,question,answer]\n",
    "input_data=pd.process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
