{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# on server: 'screen' ,then start script\n",
    "# use 'strg+a d' to return to terminal\n",
    "# use 'screen -r' to return to screen\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,LSTM\n",
    "from keras.utils import plot_model\n",
    "from keras.models import model_from_json\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import preprocess_data as ppd\n",
    "import make_RNN_models as mrm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72\n",
    "path='testpath/'\n",
    "# rnn parameters\n",
    "hidden_size = 100 #100 is the standard\n",
    "batch_size = 100 #for the training on the GPU this to be has to very large, otherwise the GPU is used very inefficiently\n",
    "epochs = 1#50\n",
    "\n",
    "size=10000\n",
    "\n",
    "#glove embedding parameters\n",
    "glove_dir = '../glove/glove.6B.100d.txt'\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86821\n",
      "86821\n",
      "86821\n",
      "86821\n"
     ]
    }
   ],
   "source": [
    "#open SQuAD-dataset and extract the relevant data from the json-file\n",
    "#to a easier readable/accessible dictionary\n",
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "train_qid=[]\n",
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer,'qid':train_qid}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        context=paragraph['context']\n",
    "        for qas in paragraph['qas']:\n",
    "            #create a dataset with only the answerable questions\n",
    "            #add a bos and eos token to the target\n",
    "            if (qas['is_impossible']==False):\n",
    "                a=context.lower()\n",
    "                b=qas['question'].lower()\n",
    "                c=qas['answers'][0]['text'].lower()\n",
    "                \n",
    "                train_new['qid'].append(qas['id'])\n",
    "                train_new['context'].append(a)\n",
    "                train_new['question'].append(b)\n",
    "                train_new['answer'].append('START_ '+c+' _END')\n",
    "#             else:\n",
    "                \n",
    "#                 a=context.lower()\n",
    "#                 b=qas['question'].lower()\n",
    "#                 c=qas['answers']\n",
    "                \n",
    "#                 train_new['qid'].append(qas['id'])\n",
    "#                 train_new['context'].append(a)\n",
    "#                 train_new['question'].append(b)\n",
    "#                 train_new['answer'].append('START_ '+str(c)+' _END')\n",
    "print(len(train_new['qid']))\n",
    "print(len(train_new['context']))\n",
    "print(len(train_new['question']))\n",
    "print(len(train_new['answer']))\n",
    "data=[train_new['context'],train_new['question'],train_new['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50676\n"
     ]
    }
   ],
   "source": [
    "#create the vocabulary for the answers\n",
    "answer_words=set()\n",
    "for line in train_new['answer']:\n",
    "    for word in line.split():\n",
    "        if word not in answer_words:\n",
    "            answer_words.add(word)\n",
    "print(len(answer_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing the maximum shapes of the data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:02<00:00,  3.18it/s]\n"
     ]
    }
   ],
   "source": [
    "data_max_shapes=ppd.get_data_max_shapes([train_new['context'],\n",
    "                                         train_new['question'],\n",
    "                                         train_new['answer']],size,answer_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'len_answer_vocab': 50676, 'max_question_len': 40, 'len_context_vocab': 42243, 'len_question_vocab': 14839, 'max_context_len': 653, 'max_answer_len': 45}\n"
     ]
    }
   ],
   "source": [
    "print(data_max_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #FIX_ME: add glove download\n",
    "# # https://nlp.stanford.edu/projects/glove/\n",
    "# #get glove embeddings\n",
    "# print('getting the glove embeddings')\n",
    "# embeddings_index = {}\n",
    "# f = open(glove_dir)\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "# f.close()\n",
    "\n",
    "# print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qid_to_answer_dict={}\n",
    "# for slice_size in range(math.ceil(len(data[0])/size)):\n",
    "#     print('training on part %s of the dataset' % slice_size)\n",
    "#     context=train_new['context'][size*slice_size:size*(slice_size+1)]\n",
    "#     question=train_new['question'][size*slice_size:size*(slice_size+1)]\n",
    "#     answer=train_new['answer'][size*slice_size:size*(slice_size+1)]\n",
    "#     data=[context,question,answer]\n",
    "#     input_data=ppd.process_data(data,data_max_shapes,answer_words)\n",
    "\n",
    "#     context_encoder_input=input_data['encoder_input']['context_encoder_input']\n",
    "#     question_encoder_input=input_data['encoder_input']['question_encoder_input']\n",
    "#     answer_decoder_input=input_data['decoder_input']['answer_decoder_input']\n",
    "#     answer_decoder_target=input_data['decoder_input']['answer_decoder_target']\n",
    "    \n",
    "#     context_token_to_int=input_data['token_to_int']['context_token_to_int']\n",
    "#     question_token_to_int=input_data['token_to_int']['question_token_to_int']\n",
    "#     answer_token_to_int=input_data['token_to_int']['answer_token_to_int']\n",
    "\n",
    "#     answer_int_to_token=input_data['int_to_token']['answer_int_to_token']\n",
    "#     ############################################################################\n",
    "#     #extract the glove-embedding to a matrix\n",
    "#     context_embedding_matrix = np.zeros((data_max_shapes['len_context_vocab'], embedding_dim))\n",
    "#     for word, i in context_token_to_int.items():\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             # words not found in embedding index will be all-zeros.\n",
    "#             context_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#     question_embedding_matrix = np.zeros((data_max_shapes['len_question_vocab'], embedding_dim))\n",
    "#     for word, i in question_token_to_int.items():\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             # words not found in embedding index will be all-zeros.\n",
    "#             question_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "#     answer_embedding_matrix = np.zeros((data_max_shapes['len_answer_vocab'], embedding_dim))\n",
    "#     for word, i in answer_token_to_int.items():\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             # words not found in embedding index will be all-zeros.\n",
    "#             answer_embedding_matrix[i] = embedding_vector\n",
    "#     embedding=[context_embedding_matrix,question_embedding_matrix,answer_embedding_matrix]\n",
    "#     ######################################################################################\n",
    "#     models=mrm.models(embedding,data_max_shapes,hidden_size,embedding_dim)\n",
    "    \n",
    "#     if os.path.isfile(path+str('train_model.h5')):\n",
    "#         print('load models from previous run')\n",
    "#         models['train_model'].load_weights(path+str('train_model.h5'))\n",
    "#         models['encoder_model'].load_weights(path+str('encoder_model.h5'))\n",
    "#         models['decoder_model'].load_weights(path+str('decoder_model.h5'))\n",
    "    \n",
    "#     print('training model')\n",
    "#     models['train_model'].fit([context_encoder_input,\n",
    "#            question_encoder_input, \n",
    "#            answer_decoder_input], \n",
    "#           answer_decoder_target,\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,)\n",
    "#     #####################################################################################\n",
    "#     print('save models')\n",
    "#     if not os.path.isdir(path):\n",
    "#         os.makedirs(path)\n",
    "#     models['train_model'].save_weights(path+str('train_model.h5')) #save weights\n",
    "#     models['encoder_model'].save_weights(path+str('encoder_model.h5')) #save weights\n",
    "#     models['decoder_model'].save_weights(path+str('decoder_model.h5')) #save weights\n",
    "    \n",
    "#     train_model_json = models['train_model'].to_json()\n",
    "#     with open(path+str('train_model.json'),'w') as json_file:\n",
    "#         json_file.write(train_model_json)\n",
    "        \n",
    "#     encoder_model_json = models['encoder_model'].to_json()\n",
    "#     with open(path+str('encoder_model.json'),'w') as json_file:\n",
    "#         json_file.write(encoder_model_json)\n",
    "        \n",
    "#     decoder_model_json = models['decoder_model'].to_json()\n",
    "#     with open(path+str('decoder_model.json'),'w') as json_file:\n",
    "#         json_file.write(decoder_model_json)\n",
    "#     #######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_to_answer_dict={}\n",
    "for slice_size in range(math.ceil(len(data[0])/size)):\n",
    "    print('training on part %s of the dataset' % slice_size)\n",
    "    context=train_new['context'][size*slice_size:size*(slice_size+1)]\n",
    "    question=train_new['question'][size*slice_size:size*(slice_size+1)]\n",
    "    answer=train_new['answer'][size*slice_size:size*(slice_size+1)]\n",
    "    data=[context,question,answer]\n",
    "    input_data=ppd.process_data(data,data_max_shapes,answer_words)\n",
    "\n",
    "    context_encoder_input=input_data['encoder_input']['context_encoder_input']\n",
    "    question_encoder_input=input_data['encoder_input']['question_encoder_input']\n",
    "    answer_decoder_input=input_data['decoder_input']['answer_decoder_input']\n",
    "    answer_decoder_target=input_data['decoder_input']['answer_decoder_target']\n",
    "    \n",
    "    context_token_to_int=input_data['token_to_int']['context_token_to_int']\n",
    "    question_token_to_int=input_data['token_to_int']['question_token_to_int']\n",
    "    answer_token_to_int=input_data['token_to_int']['answer_token_to_int']\n",
    "\n",
    "    answer_int_to_token=input_data['int_to_token']['answer_int_to_token']\n",
    "    with open('models/encoder_model.json', 'r') as encoder_json_file:\n",
    "        loaded_model_json = encoder_json_file.read()\n",
    "        encoder_model = model_from_json(loaded_model_json)\n",
    "        encoder_model.load_weights('models/encoder_model.h5')\n",
    "    \n",
    "    with open('models/decoder_model.json', 'r') as decoder_json_file:\n",
    "        loaded_model_json = decoder_json_file.read()\n",
    "        decoder_model = model_from_json(loaded_model_json)\n",
    "        decoder_model.load_weights('models/decoder_model.h5')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('SQuAD/answers.json', 'w') as file:\n",
    "#     json.dump(qid_to_answer_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('SQuAD/answers.json') as file:\n",
    "#     answers_json=json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file=path+'/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('save model')\n",
    "# if not os.path.isdir(path):\n",
    "#     os.makedirs(path)\n",
    "# model.save_weights(path+str('baseline_model.h5')) #save weights\n",
    "# model_json = model.to_json()\n",
    "# with open(path+str('baseline_model.json'),'w') as json_file:\n",
    "#     json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
