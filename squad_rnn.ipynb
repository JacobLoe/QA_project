{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import layers\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn parameters\n",
    "RNN = recurrent.LSTM\n",
    "EMBED_HIDDEN_SIZE = 50\n",
    "SENT_HIDDEN_SIZE = 100\n",
    "QUERY_HIDDEN_SIZE = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "#glove embedding parameters\n",
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open SQuAD-dataset and extract the relevatn data\n",
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        context=paragraph['context']\n",
    "        for qas in paragraph['qas']:\n",
    "            #create a dataset with only the answerable questions\n",
    "            if (qas['is_impossible']==False):\n",
    "                train_new['context'].append(context)\n",
    "                train_new['question'].append(qas['question'])\n",
    "                train_new['answer'].append(qas['answers'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate the data in on vector for preprocessing\n",
    "train_all=[]\n",
    "for line in train_new['context']:\n",
    "    train_all.append(line)\n",
    "for line in train_new['question']:\n",
    "    train_all.append(line)\n",
    "for line in train_new['answer']:\n",
    "    train_all.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the data to use as input of the rnn\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(train_all)\n",
    "context_sequences = tokenizer.texts_to_sequences(train_new['context'])\n",
    "question_sequences = tokenizer.texts_to_sequences(train_new['question'])\n",
    "answer_sequences = tokenizer.texts_to_sequences(train_new['answer'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "context = pad_sequences(context_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "question = pad_sequences(question_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "answer = pad_sequences(answer_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('Shape of context tensor:', context.shape)\n",
    "print('Shape of question tensor:', question.shape)\n",
    "print('Shape of answer tensor:', answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(context.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "context = context[indices]\n",
    "question = question[indices]\n",
    "answer = answer[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * context.shape[0])\n",
    "\n",
    "x_train_context = context[:-num_validation_samples]\n",
    "x_train_question = question[:-num_validation_samples]\n",
    "y_train_answer = answer[:-num_validation_samples]\n",
    "\n",
    "x_val_context = context[-num_validation_samples:]\n",
    "x_val_question = question[-num_validation_samples:]\n",
    "y_val_answer = answer[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the glove-embedding to a matrix\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a non-trainable embedding layer\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "\n",
    "context_layer = layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='Context_input')\n",
    "encoded_context = embedding_layer(context_layer)\n",
    "encoded_context = RNN(SENT_HIDDEN_SIZE)(encoded_context)\n",
    "\n",
    "question_layer = layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32',name='Question_input')\n",
    "encoded_question = embedding_layer(question_layer)\n",
    "encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n",
    "\n",
    "merged = layers.concatenate([encoded_context, encoded_question])\n",
    "preds = layers.Dense(1000, activation='softmax')(merged) #dimensions of dense layer have to to the same as the answer dimensions\n",
    "\n",
    "model = Model([context_layer, question_layer], preds)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "print('successfully built the model')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training')\n",
    "model.fit([x_train_context, x_train_question], y_train_answer,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.05)\n",
    "\n",
    "print('Evaluation')\n",
    "loss, acc = model.evaluate([x_val_context, x_val_question], y_val_answer,\n",
    "                           batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/baseline_total_weights.vec','w') as f:\n",
    "    print(model.get_weights(),file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
