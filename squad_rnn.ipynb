{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on server: 'screen' ,then start script\n",
    "# use 'strg+a d' to return to terminal\n",
    "# use 'screen -r' to return to screen\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Input,Dense,LSTM\n",
    "from keras.utils import plot_model\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "import process_data as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/nlp-sequence-to-sequence-networks-part-1-processing-text-data-d141a5643b72\n",
    "path='testpath/'\n",
    "# rnn parameters\n",
    "hidden_size = 100 #100 is the standard\n",
    "batch_size = 512 #for the training on the GPU this to be has to very large, otherwise the GPU is used very inefficiently\n",
    "epochs = 100\n",
    "\n",
    "size=10000\n",
    "\n",
    "#glove embedding parameters\n",
    "glove_dir = '../glove/glove.6B.100d.txt'\n",
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open SQuAD-dataset and extract the relevant data from the json-file\n",
    "#to a easier readable/accessible dictionary\n",
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        context=paragraph['context']\n",
    "        for qas in paragraph['qas']:\n",
    "            #create a dataset with only the answerable questions\n",
    "            #add a bos and eos token to the target\n",
    "            if (qas['is_impossible']==False):\n",
    "                a=context.lower()\n",
    "                b=qas['question'].lower()\n",
    "                c=qas['answers'][0]['text'].lower()\n",
    "                \n",
    "                train_new['context'].append(a)\n",
    "                train_new['question'].append(b)\n",
    "                train_new['answer'].append('START_ '+c+' _END')\n",
    "print(len(train_new['context']))\n",
    "print(len(train_new['question']))\n",
    "print(len(train_new['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context=train_new['context'][:size]\n",
    "question=train_new['question'][:size]\n",
    "answer=train_new['answer'][:size]\n",
    "data=[context,question,answer]\n",
    "input_data=pd.process_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_encoder_input=input_data['encoder_input']['context_encoder_input']\n",
    "question_encoder_input=input_data['encoder_input']['question_encoder_input']\n",
    "answer_decoder_input=input_data['decoder_input']['answer_decoder_input']\n",
    "answer_decoder_target=input_data['decoder_input']['answer_decoder_target']\n",
    "\n",
    "context_len_vocab=input_data['len_vocab']['context_len_vocab']\n",
    "question_len_vocab=input_data['len_vocab']['question_len_vocab']\n",
    "answer_len_vocab=input_data['len_vocab']['answer_len_vocab']\n",
    "\n",
    "context_token_to_int=input_data['token_to_int']['context_token_to_int']\n",
    "question_token_to_int=input_data['token_to_int']['question_token_to_int']\n",
    "answer_token_to_int=input_data['token_to_int']['answer_token_to_int']\n",
    "\n",
    "answer_int_to_token=input_data['int_to_token']['answer_int_to_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIX_ME: add glove download\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "#get glove embeddings\n",
    "embeddings_index = {}\n",
    "f = open(glove_dir)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the glove-embedding to a matrix\n",
    "context_embedding_matrix = np.zeros((context_len_vocab, embedding_dim))\n",
    "for word, i in context_token_to_int.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        context_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "question_embedding_matrix = np.zeros((question_len_vocab, embedding_dim))\n",
    "for word, i in question_token_to_int.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        question_embedding_matrix[i] = embedding_vector\n",
    "\n",
    "answer_embedding_matrix = np.zeros((answer_len_vocab, embedding_dim))\n",
    "for word, i in answer_token_to_int.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        answer_embedding_matrix[i] = embedding_vector\n",
    "print(np.shape(context_embedding_matrix),np.shape(question_embedding_matrix),np.shape(answer_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@dev.elect.iitd/neural-machine-translation-using-word-level-seq2seq-model-47538cba8cd7\n",
    "# encoder\n",
    "context_encoder_inputs = Input(shape=(None,))\n",
    "context_embedding_layer = Embedding(context_len_vocab, \n",
    "                        embedding_dim,weights=[context_embedding_matrix],trainable=False)\n",
    "context_embedding=context_embedding_layer(context_encoder_inputs)\n",
    "\n",
    "context_decoder_lstm = LSTM(embedding_dim,return_state=True)\n",
    "context_x, context_state_h, context_state_c = context_decoder_lstm(context_embedding)\n",
    "context_encoder_states = [context_state_h, context_state_c]\n",
    "\n",
    "\n",
    "question_encoder_inputs = Input(shape=(None,))\n",
    "question_embedding_layer = Embedding(question_len_vocab, \n",
    "                    embedding_dim,weights=[question_embedding_matrix],trainable=False)\n",
    "question_embedding=question_embedding_layer(question_encoder_inputs)\n",
    "\n",
    "question_decoder_lstm = LSTM(embedding_dim,return_state=True)\n",
    "question_x, question_state_h, question_state_c = question_decoder_lstm(question_embedding)\n",
    "question_encoder_states = [question_state_h, question_state_c]\n",
    "\n",
    "\n",
    "encoder_state_h=Concatenate()([context_state_h,question_state_h])\n",
    "encoder_state_c=Concatenate()([context_state_c,question_state_c])\n",
    "concat_encoder_states=[encoder_state_h,encoder_state_c]\n",
    "\n",
    "# decoder #################################\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "answer_embedding_layer = Embedding(answer_len_vocab, \n",
    "                             embedding_dim,weights=[answer_embedding_matrix])\n",
    "answer_embedding = answer_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(embedding_dim*2, return_sequences=True,return_state=True)\n",
    "decoder_lstm_output,_,_ = decoder_lstm(answer_embedding, initial_state=concat_encoder_states)\n",
    "\n",
    "decoder_dense = Dense(answer_len_vocab, activation='softmax')\n",
    "decoder_output = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model([context_encoder_inputs,question_encoder_inputs, decoder_inputs], decoder_output)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([context_encoder_input,\n",
    "           question_encoder_input, \n",
    "           answer_decoder_input], \n",
    "          answer_decoder_target,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights('models/baseline/baseline_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the encoder model \n",
    "encoder_model = Model([context_encoder_inputs,question_encoder_inputs],concat_encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(hidden_size*2,))\n",
    "decoder_state_input_c = Input(shape=(hidden_size*2,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "final_dex2= answer_embedding_layer(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2,)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_model.predict([input_data['encoder_input']['encoder_input_context'][0:1],\n",
    "#                        input_data['encoder_input']['encoder_input_question'][0:1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(context_input_seq,question_input_seq,):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict([context_input_seq,question_input_seq])\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = answer_token_to_int['START_']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = answer_int_to_token[sampled_token_index]\n",
    "        decoded_sentence += ' '+sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '_END' or\n",
    "           len(decoded_sentence) > 52):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(10):\n",
    "    context_input_seq = input_data['encoder_input']['context_encoder_input'][seq_index:seq_index+1]\n",
    "    question_input_seq = input_data['encoder_input']['question_encoder_input'][seq_index:seq_index+1]\n",
    "    \n",
    "    decoded_sentence = decode_sequence(context_input_seq,question_input_seq)\n",
    "    print('-')\n",
    "    print('context: ',train_new['context'][seq_index:seq_index+1],'\\n')\n",
    "    print('question: ',train_new['question'][seq_index:seq_index+1],'\\n')\n",
    "    print('answer: ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_model(model, to_file=path+'/model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('save model')\n",
    "# if not os.path.isdir(path):\n",
    "#     os.makedirs(path)\n",
    "# model.save_weights(path+str('baseline_model.h5')) #save weights\n",
    "# model_json = model.to_json()\n",
    "# with open(path+str('baseline_model.json'),'w') as json_file:\n",
    "#     json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
