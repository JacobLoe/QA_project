{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from scipy.spatial import distance\n",
    "import numpy as np\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import tqdm\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from bert_embedding import BertEmbedding\n",
    "from scipy import spatial\n",
    "import mxnet as mx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchy of the word to be selected in question.\n",
    "def hierarchy(a, b, c):\n",
    "    # a = nn_cnt / b = vb_cnt / c = ad_cnt\n",
    "\n",
    "    if a == b and b == c:\n",
    "        return 'noun', 'verb', 'adj'\n",
    "\n",
    "    if a > b:\n",
    "        if b > c:\n",
    "            return 'adj', 'verb', 'noun'\n",
    "        else:\n",
    "            return 'verb', 'adj', 'noun'\n",
    "\n",
    "    elif b >= a:\n",
    "        if a > c:\n",
    "            return 'adj', 'noun', 'verb'\n",
    "        else:\n",
    "            return 'noun', 'adj', 'verb'\n",
    "\n",
    "    elif c >= a:\n",
    "        if a > b:\n",
    "            return 'verb', 'noun', 'adj'\n",
    "        else:\n",
    "            return 'noun', 'verb', 'adj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat the file and save it as 'embedding'\n",
    "#glove2word2vec('counter-fitted-vectors.txt', 'embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the vectors in model\n",
    "model = KeyedVectors.load_word2vec_format('embedding', binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of english stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of POS\n",
    "# VB verb, base form take, VBD verb, past tense tookVBG verb, gerund/present participle takingVBN verb, past participle takenVBP verb, sing. present, non-3d takeVBZ verb, 3rd perso sing. present takes\n",
    "# JJ adjective ‘big’ JJR adjective, comparative ‘bigger’ JJS adjective, superlative ‘biggest’\n",
    "\n",
    "\n",
    "pos = {'noun' : set(['NN','NNS']),\n",
    "       'verb' : set(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ' ]),\n",
    "       'adj' : set(['JJ', 'JJR', 'JJS'])}\n",
    "\n",
    "nn_cnt = 0\n",
    "vb_cnt = 0\n",
    "ad_cnt = 0\n",
    "\n",
    "# load bert model\n",
    "bert = BertEmbedding(max_seq_length=65)\n",
    "\n",
    "failed = 0\n",
    "total = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../SQuAD/train-v2.0(1).json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f94a631c98cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../SQuAD/train-v2.0(1).json'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrun_nr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_nr\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../SQuAD/train-v2.0(1).json'"
     ]
    }
   ],
   "source": [
    "with open('../SQuAD/train-v2.0(1).json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for run_nr, block in enumerate(data['data']):\n",
    "        if run_nr<0:\n",
    "            continue\n",
    "        else:\n",
    "            for paragraph in block['paragraphs']:\n",
    "                for qa in paragraph['qas']:\n",
    "                    # to break if nothing in the sentence can be changed\n",
    "                    error = False\n",
    "                    total += 1\n",
    "\n",
    "                    # get a sentence\n",
    "                    sent = qa['question']\n",
    "                    # tokenize and POS-tag the sentence\n",
    "                    tkn_sent = word_tokenize(sent)\n",
    "\n",
    "                    # pos tag the sentence\n",
    "                    pos_sent = [(word.lower(), tag) for word, tag in pos_tag(tkn_sent)]\n",
    "                    #print(pos_sent)\n",
    "\n",
    "                    # select word to be replace given the hierachy provided:\n",
    "                    # prioritize grammatical classes by counting the frequency in the instances.\n",
    "                    pos_order = hierarchy(nn_cnt, vb_cnt, ad_cnt)\n",
    "\n",
    "                    # tags sorted by priority\n",
    "                    top = pos[pos_order[0]]\n",
    "                    mid = pos[pos_order[1]]\n",
    "                    fin = pos[pos_order[2]]\n",
    "\n",
    "                    # find fitting POS tags\n",
    "                    top_col, mid_col, fin_col = [],[],[]\n",
    "                    for idx, inst in enumerate(pos_sent):\n",
    "                        # to get into the selection the instance has to be in the embedding dictionary and not a stopword\n",
    "                        if model.__contains__(inst[0]) and not (inst[0] in stop_words):\n",
    "                            if inst[1] in top:\n",
    "                                top_col.append(idx)\n",
    "                            elif inst[1] in mid:\n",
    "                                mid_col.append(idx)\n",
    "                            elif inst[1] in fin:\n",
    "                                fin_col.append(idx)\n",
    "\n",
    "                    # pick randomly a word according to hierarchy\n",
    "                    if len(top_col)>0:\n",
    "                        chosen_idx = random.choice(top_col)\n",
    "                    elif len(mid_col)>0:\n",
    "                        chosen_idx = random.choice(mid_col)\n",
    "                    elif len(fin_col)>0:\n",
    "                        chosen_idx = random.choice(fin_col)\n",
    "                    else:\n",
    "                        error = True\n",
    "                        failed += 1\n",
    "                        #print('ERROR: couldnt replace anything', '\\n')\n",
    "\n",
    "                    if error == False:\n",
    "                        # chosen word and its POS\n",
    "                        chosen_word = pos_sent[chosen_idx][0]\n",
    "                        chosen_pos = pos_sent[chosen_idx][1]\n",
    "\n",
    "                        # the chosen word/pos pair\n",
    "                        if chosen_pos in pos['noun']:\n",
    "                            nn_cnt +=1\n",
    "                        elif chosen_pos in pos['verb']:\n",
    "                            vb_cnt +=1\n",
    "                        elif chosen_pos in pos['adj']:\n",
    "                            ad_cnt +=1\n",
    "\n",
    "                        # distance to all other words in the embedding dict\n",
    "                        omega = 0.9\n",
    "                        mst_smlr = model.most_similar(chosen_word)\n",
    "                        new_words = [word for word, score in mst_smlr if score<omega]\n",
    "\n",
    "                        if len(new_words)==0:\n",
    "                            failed += 1\n",
    "                            #print('ERROR: couldnt find good alternatives for ', chosen_word, '\\n')\n",
    "                        else:\n",
    "                            #print(chosen_word, new_words, '\\n')\n",
    "\n",
    "                            # get all possible alternative sentences\n",
    "                            sentence_options = [re.sub(r\"\\s's\", \" s\", \" \".join(tkn_sent))]\n",
    "                            for word in new_words:\n",
    "                                tkn_sent[chosen_idx] = word\n",
    "                                sentence_options.append(re.sub(r\"\\s's\", \" s\", ' '.join(tkn_sent[:])))\n",
    "\n",
    "                            # calculate bert sentence embeddings\n",
    "                            results = bert.embedding(sentence_options)\n",
    "                            \n",
    "                            # calculate similarity scores between suggested and original sentence\n",
    "                            similarity = []\n",
    "                            try:\n",
    "                                og_wrd_embed = results[0][1][chosen_idx]\n",
    "                            except IndexError:\n",
    "                                print(\"Failed: \", failed)\n",
    "                                print(\"Total: \", total)\n",
    "                                print(\"total nouns: \", nn_cnt)\n",
    "                                print(\"total verbs: \", vb_cnt)\n",
    "                                print(\"total adj: \", ad_cnt)\n",
    "\n",
    "\n",
    "                            for sug_sent in results[1:]:\n",
    "                                similarity.append(1 - spatial.distance.cosine(og_wrd_embed, sug_sent[1][chosen_idx]))\n",
    "\n",
    "                            # pick the best new sentence\n",
    "                            max_value = max(similarity)\n",
    "                            best_sent_idx = similarity.index(max_value)+1\n",
    "                            qa['question'] = sentence_options[best_sent_idx][:]\n",
    "\n",
    "\n",
    "        with open('../SQuAD/data_new2.json', 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "            print('saved after block ', run_nr,\"/442\")\n",
    "    print(\"done\")\n",
    "    print(\"Failed: \", failed)\n",
    "    print(\"Total: \", total)\n",
    "    print(\"total nouns: \", nn_cnt)\n",
    "    print(\"total verbs: \", vb_cnt)\n",
    "    print(\"total adj: \", ad_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
