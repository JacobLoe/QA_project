{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code to download SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "# each paragraph is a different topic indicated by the title\n",
    "# each topic contains a number of different short passages related to the topic, the context\n",
    "# for each topic there are ~10 question/answer pairs, and an indicator if a question is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        context=tokenize(paragraph['context'])\n",
    "        for qas in paragraph['qas']:\n",
    "            #create a dataset with only the answerable questions\n",
    "            if (qas['is_impossible']==False):\n",
    "                train_new['context'].append(context)\n",
    "                train_new['question'].append(tokenize(qas['question']))\n",
    "                train_new['answer'].append(tokenize(qas['answers'][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96585,)\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for context,question,answer in zip(train_new['context'],train_new['question'],train_new['answer']):\n",
    "#     print(type(context))\n",
    "#     print(type(question))\n",
    "#     print(type(answer))\n",
    "#     vocab |= set(context+question+[str(answer)])\n",
    "    vocab |= set(context+question+answer)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1\n",
    "print(np.shape(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "#needed for sequence padding\n",
    "story_maxlen = max(map(len, (x for x, _, _ in zip(train_new['context'],train_new['question'],train_new['answer']))))\n",
    "query_maxlen = max(map(len, (x for _, x, _ in zip(train_new['context'],train_new['question'],train_new['answer']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# print(len(word_idx))\n",
    "print(story_maxlen)\n",
    "print(query_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_stories(data, word_idx, context_maxlen, question_maxlen):\n",
    "    xs = []\n",
    "    xqs = []\n",
    "    ys = []\n",
    "    for context, question, answer in data:\n",
    "        x = [word_idx[w] for w in context]\n",
    "        xq = [word_idx[w] for w in question]\n",
    "        # let's not forget that index 0 is reserved\n",
    "        y = 4#np.zeros(len(word_idx) + 1)\n",
    "#         for string in answer:\n",
    "#             y[word_idx[string]] = 1\n",
    "#         print(np.shape(y))\n",
    "#         print(y)\n",
    "#         break\n",
    "        xs.append(x)\n",
    "        xqs.append(xq)\n",
    "        ys.append(y)\n",
    "    return [pad_sequences(xs, maxlen=context_maxlen),\n",
    "            pad_sequences(xqs, maxlen=question_maxlen), np.array(ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
