{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add code to download SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SQuAD/train-v2.0.json') as file:\n",
    "    train=json.load(file)\n",
    "# each paragraph is a different topic indicated by the title\n",
    "# each topic contains a number of different short passages related to the topic, the context\n",
    "# for each topic there are ~10 question/answer pairs, and an indicator if a question is impossible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "train_context=[]\n",
    "train_question=[]\n",
    "train_answer=[]\n",
    "train_new={'context':train_context,'question':train_question,'answer':train_answer}\n",
    "for j,data in enumerate(train['data']):\n",
    "    for i,paragraph in enumerate(data['paragraphs']):\n",
    "        context=tokenize(paragraph['context'])\n",
    "        for qas in paragraph['qas']:\n",
    "            #create a dataset with only the answerable questions\n",
    "            if (qas['is_impossible']==False):\n",
    "                train_new['context'].append(context)\n",
    "                train_new['question'].append(tokenize(qas['question']))\n",
    "                train_new['answer'].append(tokenize(qas['answers'][0]['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86821,)\n",
      "(260463,)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_new['context']))\n",
    "train_all=[]\n",
    "for line in train_new['context']:\n",
    "    train_all.append(line)\n",
    "for line in train_new['question']:\n",
    "    train_all.append(line)\n",
    "for line in train_new['answer']:\n",
    "    train_all.append(line)\n",
    "print(np.shape(train_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96585,)\n"
     ]
    }
   ],
   "source": [
    "vocab = set()\n",
    "for context,question,answer in zip(train_new['context'],train_new['question'],train_new['answer']):\n",
    "#     print(type(context))\n",
    "#     print(type(question))\n",
    "#     print(type(answer))\n",
    "#     vocab |= set(context+question+[str(answer)])\n",
    "    vocab |= set(context+question+answer)\n",
    "vocab = sorted(vocab)\n",
    "vocab_size = len(vocab) + 1\n",
    "print(np.shape(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "#needed for sequence padding\n",
    "context_maxlen = max(map(len, (x for x, _, _ in zip(train_new['context'],train_new['question'],train_new['answer']))))\n",
    "question_maxlen = max(map(len, (x for _, x, _ in zip(train_new['context'],train_new['question'],train_new['answer']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "# print(len(word_idx))\n",
    "print(context_maxlen)\n",
    "print(question_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf\n",
    "import nltk\n",
    "# nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 5000\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(brown.sents())\n",
    "\n",
    "tfidf = tokenizer.texts_to_matrix(brown.sents(), 'tfidf').T\n",
    "tfidf /= np.linalg.norm(tfidf, axis= -1, keepdims= True) + 1e-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = lambda w1, w2: tfidf[tokenizer.word_index[w1]] @ tfidf[tokenizer.word_index[w2]]\n",
    "test = lambda f, ws: f(*[tfidf[tokenizer.word_index[w]] for w in ws])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57340\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "print(len(brown.sents()))\n",
    "print(len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cbow\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = 3\n",
    "seed = 0\n",
    "dim = 200\n",
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = [ngram\n",
    "          for sent in tokenizer.texts_to_sequences(brown.sents())\n",
    "          for ngram in zip(*[sent[i:] for i in range(context + 1 + context)])]\n",
    "x = np.array([n[:context] + n[-context:] for n in ngrams])\n",
    "y = np.array([n[context] for n in ngrams])\n",
    "del ngrams\n",
    "print(len(x))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9508740\n",
      "9508740\n"
     ]
    }
   ],
   "source": [
    "# ngrams = [ngram\n",
    "#           for sent in tokenizer.texts_to_sequences(train_all)\n",
    "#           for ngram in zip(*[sent[i:] for i in range(context + 1 + context)])]\n",
    "# x = np.array([n[:context] + n[-context:] for n in ngrams])\n",
    "# y = np.array([n[context] for n in ngrams])\n",
    "# del ngrams\n",
    "# print(len(x))\n",
    "# print(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(x)\n",
    "# np.random.seed(seed)\n",
    "# np.random.shuffle(y)\n",
    "\n",
    "# x_valid = x[:4096]\n",
    "# y_valid = y[:4096]\n",
    "# x_train = x[4096:]\n",
    "# y_train = y[4096:]\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Embedding(vocab_size, dim, input_length= 2*context))\n",
    "# model.add(Lambda(partial(K.mean, axis= 1)))\n",
    "# model.add(Dense(vocab_size, activation= 'softmax')) \n",
    "# model.compile(loss= 'sparse_categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# model.fit(x_train, y_train, validation_data= (x_valid, y_valid), epochs= 6, batch_size= 256, shuffle= False)\n",
    "\n",
    "\n",
    "# embed = model.get_weights()[0]\n",
    "\n",
    "# with open(\"embeddings1.vec\", 'w') as f:\n",
    "#     print(vocab_size-1, dim, file= f)\n",
    "#     for i in range(1, vocab_size):\n",
    "#         print(tokenizer.index_word[i], \" \".join(map(str, embed[i])), file= f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999\n",
      "the\n",
      ",\n",
      ".\n",
      "of\n"
     ]
    }
   ],
   "source": [
    "# np.shape(model.get_weights()[2])\n",
    "with open('embeddings1.vec') as f:\n",
    "    for i,line in enumerate(f):\n",
    "        values=line.split()\n",
    "        print(values[0])\n",
    "        if i==4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
